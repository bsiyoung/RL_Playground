# Multi Armed Bandit (MAB) Problems

## 1. Simple Bandit Algorithm

## 2. Weighted Average

## 3. Optimistic Initial Values

## 4. Upper Confidence Bound (UCB)
